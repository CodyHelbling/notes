<!--- Question --->
What's the point of Apache Spark?
<!--- Answer --->
It can help analyze and process large amounts of data for business or other goals.

<!--- Question --->
What are some common data processing tasks?
<!--- Answer --->
- Parsing fields from text
- Accounting for missing values
- Identifying and investigating anomalies
- Summarizing using tables and charts


<!--- Question --->
How should tools be chosen for data munging?
<!--- Answer --->
- Small messy data (growing 10-1000 rows per day)
    - Excel
- Medium Sized data with high integrity
    - SQL, Python, Java to build data and ML applications
- Big data that is messy and stored across different machines in different formats
    - Hadoop, MapReduce, Spark

<!--- Question --->
What can you do with Spark?
<!--- Answer --->
- Explore data
    - REPL
- Clean and prepare data
- Apply machine learning
- Build data applications
- Distributed computing

<!--- Question --->
What is a Resilient Distributed Dataset?
<!--- Answer --->
- The main programming abstraction in Spark.
- In memory collections of objects
- Held across multiple machines which allows you to interact and play with billions of rows of data without caring about the complexities

<!--- Question --->
What are Spark's architectural components?
<!--- Answer --->
1. Spark core
    - a computing system
2. Storage System
3. Cluster Manager

<!--- Question --->

<!--- Answer --->